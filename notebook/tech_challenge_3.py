# -*- coding: utf-8 -*-
"""tech_challenge_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dV-3jrW8Mru6HEU3hYF3wNvlOAIptLGJ
"""

# === Parte 1 ===
# Objetivo: montar o Drive e instalar dependências básicas.
# Comentários:
# 1. Montar drive para acessar dados e salvar modelos.
# 2. Instalar pip-tools (opcional) e compilar requirements.
# 3. Recomendo rodar instalação de dependências uma vez por sessão.

from google.colab import drive
drive.mount('/content/drive')

!pip install pip-tools

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.in
# transformers
# accelerate
# bitsandbytes
# sentencepiece
# datasets
# peft==0.9.0
# trl==0.8.0
# pyarrow
# jedi
# pandas
# requests
# numpy
# torch

!pip-compile requirements.in
!ls

!pip install -r requirements.txt

# === CÉLULA 2 ===
# Objetivo: checar GPU e ambiente PyTorch.
# Comentários:
# 1. Útil pra confirmar se usar bfloat16 / bf16 / 4-bit faz sentido.
# 2. Se GPU não disponível, ajustar quantization / dtype.

import torch, sys, transformers, trl, bitsandbytes, peft
print("Python:", sys.executable)
print("Transformers:", transformers.__version__)
print("TRL:", trl.__version__)
print("torch:", torch.__version__)
print("PEFT:", peft.__version__)
print("bnb:", bitsandbytes.__version__)
print("GPU disponível:", torch.cuda.is_available())
if torch.cuda.is_available():
    try:
        print("Nome da GPU:", torch.cuda.get_device_name(0))
    except Exception:
        print("Não foi possível recuperar o nome da GPU (em alguns ambientes isso falha).")

# acelera em entradas de tamanho constante (opcional)
torch.backends.cudnn.benchmark = True

# === CÉLULA 3 (ATUALIZADA) ===
# Objetivo: carregar dataset com fallback para SSD local (/tmp) para acelerar I/O.

from pathlib import Path
import shutil, json
from datasets import Dataset

# Se você já definiu DRIVE_ROOT antes, reaproveita; caso contrário define aqui:
try:
    DRIVE_ROOT
except NameError:
    DRIVE_ROOT = Path("/content/drive/MyDrive/tech-challenge-3")

drive_dataset_path = DRIVE_ROOT / "data" / "dataset_formatado_para_treino.json"
local_dataset_copy = Path("/tmp/dataset_formatado_para_treino.json")

# Copia do Drive para /tmp (se ainda não copiado)
if not local_dataset_copy.exists():
    try:
        print("Copiando dataset do Drive para /tmp ...")
        shutil.copy(drive_dataset_path, local_dataset_copy)
        print("Cópia concluída:", local_dataset_copy)
    except Exception as e:
        print("Falha ao copiar para /tmp:", e)

# Usa /tmp se existir; senão, usa o Drive
DATAFILE = str(local_dataset_copy if local_dataset_copy.exists() else drive_dataset_path)
print("Lendo dataset de:", DATAFILE)

# Carrega JSON/JSONL
examples = []
with open(DATAFILE, "r", encoding="utf-8") as f:
    first = f.readline()
    if first.strip().startswith("["):
        f.seek(0); data = json.load(f); examples = data
    else:
        # JSONL
        if first.strip():
            examples.append(json.loads(first))
        for line in f:
            if line.strip():
                examples.append(json.loads(line))

ds = Dataset.from_list(examples)
print("Dataset criado:", len(ds), "linhas. Colunas:", ds.column_names)
assert "text" in ds.column_names, "Esperava coluna 'text'."

# (opcional) criar amostra para debug rápido
ds_reduzido = ds.shuffle(seed=42).select(range(min(105_000, len(ds))))

# === CÉLULA 4 (ATUALIZADA) ===
# Objetivo: selecionar o split de treino (usar ds_reduzido) e, opcionalmente, criar validação.

from datasets import DatasetDict

USE_REDUZIDO = True  # <- deixe True para treinar no reduzido

if USE_REDUZIDO:
    assert 'ds_reduzido' in globals(), "ds_reduzido não encontrado; verifique a CÉLULA 3."
    ds_train = ds_reduzido
    print("Usando ds_reduzido como treino:", len(ds_train))
else:
    assert 'ds' in globals(), "ds não encontrado; verifique a CÉLULA 3."
    ds_train = ds
    print("Usando ds completo como treino:", len(ds_train))

# (opcional) criar um pequeno conjunto de validação para monitorar overfitting
MAKE_VAL = False
if MAKE_VAL:
    split = ds_train.train_test_split(test_size=0.01, seed=42)
    ds_train, ds_eval = split["train"], split["test"]
    print("Validação criada:", len(ds_eval))
else:
    ds_eval = None

# === CÉLULA 5 ===
# Objetivo: login no Hugging Face (necessário para push/pull de modelos)
# Comentários:
# 1. Insira token com hf_...
# 2. Considere usar !huggingface-cli login em ambientes onde input() falha.

from huggingface_hub import login, whoami
token = input("Cole aqui seu HF token (começa com hf_...): ").strip()
login(token=token)
print("Usuário HF:", whoami().get("name"))

# === CÉLULA 6 ===
# Objetivo: teste baseline / inferência (carregar modelo em modo 4-bit quando possível)
# Comentários:
# 1. Use isto para ver saída do modelo antes do fine-tuning.
# 2. Ajuste MODEL_ID conforme necessário.

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

MODEL_ID = "TinyLlama/TinyLlama_v1.1"   # substituir se necessário

# 6.1 - tentar configurar quantização 4-bit
use_4bit = True
try:
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    )
except Exception as e:
    print("Erro ao criar BitsAndBytesConfig:", e)
    use_4bit = False

print("Use 4-bit?", use_4bit)
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# 6.2 - carregar modelo (quantizado se possível)
if use_4bit:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        quantization_config=bnb_config,
        trust_remote_code=False,
    )
else:
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=False,
    )

model.config.use_cache = True

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
prompt = "Gere uma descrição criativa para o produto: Título: Girls Ballet Tutu Neon Pink"
out = pipe(prompt, max_new_tokens=120, do_sample=True, temperature=0.7)[0]["generated_text"]
print("Saída exemplo:\n", out)

# === CÉLULA 7 (LIMPA) ===
# Objetivo: definir configuração global usada por todo o notebook.

from transformers import AutoTokenizer

MODEL_ID  = "TinyLlama/TinyLlama_v1.1"
MAX_LEN   = 128   # ajuste aqui; será respeitado na Célula 10 (pré-tokenização/packing)

def get_tokenizer(model_id: str = MODEL_ID):
    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token
    tok.padding_side = "right"
    return tok

print("Config OK -> MODEL_ID:", MODEL_ID, "| MAX_LEN:", MAX_LEN)

# === CÉLULA 8 ===
# Objetivo: (opcional) reduzir dataset para testes rápidos
# Comentários:
# 1. Use ds_reduzido para debug/validação antes do treino completo.
# 2. Ao treinar de verdade, substitua por ds completo.

print("Tamanho total do dataset:", len(ds))

# Para teste rápido, pegue uma amostra (ex.: 105k ou menos se dataset for menor)
ds_reduzido = ds.shuffle(seed=42).select(range(min(105000, len(ds))))
print("Amostra de treino:", len(ds_reduzido))

# === CÉLULA 9 (LIMPA) ===
# Objetivo: configurar cache/ambiente HF (sem carregar modelo aqui).

import os, pathlib, torch
HF_HOME = "/content/drive/MyDrive/tech-challenge-3/LLM"
pathlib.Path(HF_HOME).mkdir(parents=True, exist_ok=True)

os.environ["HF_HOME"] = HF_HOME
os.environ["HUGGINGFACE_HUB_CACHE"] = HF_HOME
os.environ["TRANSFORMERS_CACHE"] = HF_HOME

print("HF cache configurado em:", HF_HOME)

# === CÉLULA 10 — treino com checkpoints, logs, versão final, pré-tokenização + packing,
#                  persistent_workers, prefetch_factor e ajustes de performance ============

import os, sys, csv, json, time, math, datetime
from pathlib import Path
import torch
from torch.utils.data import DataLoader
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments,
    Trainer, TrainerCallback, TrainerControl, TrainerState, default_data_collator
)
from transformers.trainer_utils import get_last_checkpoint
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import trl
from datasets import Dataset

# --------- Versões (tolerante) ----------
def _safe_ver(modname):
    try:
        m = __import__(modname)
        return getattr(m, "__version__", "unknown")
    except Exception:
        return "not-imported"

print("Python:", sys.executable)
print("torch:", torch.__version__)
print("trl:", getattr(trl, "__version__", "unknown"))
print("peft:", _safe_ver("peft"))
print("bitsandbytes:", _safe_ver("bitsandbytes"))

# --- Reuso de config da CÉLULA 7 ---
assert 'MODEL_ID' in globals(), "Defina MODEL_ID na CÉLULA 7."
assert 'MAX_LEN'  in globals(), "Defina MAX_LEN na CÉLULA 7."
# reutiliza tokenizer se já existir; senão, cria via get_tokenizer() da CÉLULA 7
try:
    tokenizer  # já existe?
except NameError:
    try:
        tokenizer = get_tokenizer(MODEL_ID)  # função definida na CÉLULA 7
    except NameError:
        # fallback defensivo (só se a CÉLULA 7 não tiver sido executada)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"
print("Tokenizer OK (reuso da CÉLULA 7).")


# --------- Config geral ----------
OUT_DIR    = Path("/content/drive/MyDrive/tech-challenge-3/LLM/ft-output")
ADAPTERDIR = Path("/content/drive/MyDrive/tech-challenge-3/LLM/tinyllama-lora")
OUT_DIR.mkdir(parents=True, exist_ok=True)
ADAPTERDIR.mkdir(parents=True, exist_ok=True)
ckpt_dir = OUT_DIR / "full"
ckpt_dir.mkdir(parents=True, exist_ok=True)

# CPUs/workers
n_cpus = os.cpu_count() or 2
recommended = max(1, min(n_cpus - 1, 8))
dataloader_num_workers = recommended
print(f"CPUs detectadas: {n_cpus} | dataloader_num_workers: {dataloader_num_workers} (base recomendada: {recommended})")

# Boosts de kernel
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# --------- Tokenizer ----------
# vem da celula 7

# --------- Dataset de treino (vem da CÉLULA 4) ----------
assert 'ds_train' in globals(), "ds_train não definido. Rode a CÉLULA 4 antes."
print("Treinando com", len(ds_train), "exemplos (ds_train)")

# --------- QLoRA (bitsandbytes) ----------
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
)

# --------- Modelo base ----------
print("Carregando modelo base (quantizado se possível)...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    quantization_config=bnb_config,
    trust_remote_code=False,
)
base_model.config.use_cache = False

# gradient checkpointing
if hasattr(base_model, "gradient_checkpointing_enable"):
    try:
        base_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={"use_reentrant": False})
    except TypeError:
        base_model.gradient_checkpointing_enable()

# preparar p/ kbit
if getattr(base_model, "is_loaded_in_4bit", False) or getattr(base_model, "is_loaded_in_8bit", False):
    try:
        base_model = prepare_model_for_kbit_training(base_model)
        print("prepare_model_for_kbit_training applied.")
    except Exception as e:
        print("Aviso: prepare_model_for_kbit_training falhou:", e)

# usar SDPA quando possível (Transformers recentes)
try:
    base_model.config.attn_implementation = "sdpa"
except Exception:
    pass

# --------- LoRA ----------
peft_config = LoraConfig(
    r=16, lora_alpha=16, lora_dropout=0.05, bias="none",
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    task_type="CAUSAL_LM",
)
model = get_peft_model(base_model, peft_config)
# redimensiona a tabela de embeddings para caber os tokens extras
model.resize_token_embeddings(len(tokenizer))

for n, p in model.named_parameters():
    if "lora_" in n:
        p.requires_grad = True
print("PEFT model criado. LoRA camadas ativas.")

# --------- Pré-tokenização ----------
PRETOKENIZE = True

def pretokenize_dataset(ds_src, tokenizer, text_field="text", max_length=MAX_LEN, batched=True, batch_size=4096):
    def tokenize_batch(examples):
        return tokenizer(examples[text_field], truncation=True, max_length=max_length, padding="max_length")
    print("Pré-tokenizando dataset (pode demorar)...")
    ds_tok = ds_src.map(tokenize_batch, batched=batched, batch_size=batch_size,
                        remove_columns=[c for c in ds_src.column_names if c != text_field])
    # labels = input_ids (CLM)
    def _add_labels(batch):
        return {"labels": batch["input_ids"]}
    ds_tok = ds_tok.map(_add_labels, batched=True)
    # remover texto cru (economia de RAM)
    ds_tok = ds_tok.remove_columns([text_field])
    # tensores
    ds_tok.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
    print("Pré-tokenização completa. Exemplos:", len(ds_tok))
    return ds_tok

if PRETOKENIZE:
    ds_train_tok = pretokenize_dataset(ds_train, tokenizer, text_field="text", max_length=MAX_LEN)
else:
    ds_train_tok = None

# --------- Constant-length packing ----------
def gen_packed_chunks(src_ds, chunk_size, eos_id):
    buf = []
    for rec in src_ds:
        am = rec["attention_mask"]
        ii = rec["input_ids"]
        # comprimento válido
        valid = int(am.sum().item()) if torch.is_tensor(am) else int(sum(am))
        # sequência válida como lista
        seq = ii[:valid].tolist() if torch.is_tensor(ii) else ii[:valid]
        # adiciona EOS e acumula
        buf.extend(seq); buf.append(int(eos_id))
        # emite blocos completos
        while len(buf) >= chunk_size:
            chunk = buf[:chunk_size]; buf = buf[chunk_size:]
            yield {"input_ids": chunk, "attention_mask": [1]*chunk_size, "labels": chunk.copy()}

if PRETOKENIZE:
    print(f"Empacotando em blocos de {MAX_LEN} tokens...")
    eos_id = int(tokenizer.eos_token_id)
    packed_ds = Dataset.from_generator(lambda: gen_packed_chunks(ds_train_tok, MAX_LEN, eos_id))
    packed_ds.set_format(type="torch", columns=["input_ids","attention_mask","labels"])
    print("packed_ds len:", len(packed_ds))
    train_dataset = packed_ds
else:
    train_dataset = ds_train

# --------- Hiperparâmetros + steps ----------
per_device_bs = 8     # se der OOM, reduzir para 6/4
grad_accum    = 8     # batch efetivo = per_device_bs * grad_accum
world_size    = 1
N = len(train_dataset)
steps_per_epoch = math.ceil(N / (per_device_bs * grad_accum * world_size))
save_steps = max(100, steps_per_epoch // 20)
print(f"N={N} | steps/epoch={steps_per_epoch} | save_steps={save_steps}")

# --------- Callbacks ----------
class SavePeftCallback(TrainerCallback):
    """Salva somente o adapter LoRA periodicamente (sem tokenizer para reduzir I/O)."""
    def __init__(self, save_dir: str, save_steps: int):
        self.save_dir = Path(save_dir); self.save_steps = save_steps
        self.save_dir.mkdir(parents=True, exist_ok=True)
    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        if state.global_step and state.global_step % self.save_steps == 0:
            model = kwargs.get("model")
            if model is not None:
                ckpt_path = self.save_dir / f"lora-ckpt-step-{state.global_step}"
                ckpt_path.mkdir(parents=True, exist_ok=True)
                try:
                    model.save_pretrained(str(ckpt_path))
                    print(f"[SavePeftCallback] Saved LoRA checkpoint to {ckpt_path}")
                except Exception as e:
                    print(f"[SavePeftCallback] erro ao salvar LoRA: {e}")
        return control

class CSVLoggerCallback(TrainerCallback):
    """Grava logs periódicos em CSV (global_step, epoch, time, loss, learning_rate, ...)."""
    def __init__(self, csv_path: str):
        self.csv_path = Path(csv_path); self._header_written = self.csv_path.exists()
        self.csv_path.parent.mkdir(parents=True, exist_ok=True)
    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):
        if not logs: return control
        row = {
            "global_step": getattr(state, "global_step", None),
            "epoch": getattr(state, "epoch", None),
            "time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()),
            "loss": logs.get("loss", logs.get("loss_step", None)),
            "learning_rate": logs.get("learning_rate", None),
        }
        extra = {k: v for k, v in logs.items() if k not in ("loss","learning_rate","epoch")}
        extras_keys = sorted(extra.keys())
        fieldnames = ["global_step","epoch","time","loss","learning_rate"] + extras_keys
        try:
            write_header = False
            if not self._header_written:
                write_header = True; self._header_written = True
            with open(self.csv_path, "a", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=fieldnames)
                if write_header: w.writeheader()
                for k in extras_keys: row[k] = extra.get(k)
                w.writerow(row)
        except Exception as e:
            print(f"[CSVLoggerCallback] erro ao escrever CSV: {e}")
        return control

# --------- TrainingArguments ----------
use_bf16 = False
try:
    use_bf16 = torch.cuda.is_bf16_supported()
except Exception:
    pass

training_args = TrainingArguments(
    output_dir=str(ckpt_dir),
    per_device_train_batch_size=per_device_bs,
    gradient_accumulation_steps=grad_accum,
    learning_rate=2e-4,
    lr_scheduler_type="linear",
    logging_strategy="steps",
    logging_steps=50,
    save_strategy="steps",
    save_steps=save_steps,
    save_total_limit=5,
    num_train_epochs=1,
    dataloader_num_workers=dataloader_num_workers,
    dataloader_pin_memory=True,
    remove_unused_columns=False,
    fp16=(not use_bf16) and torch.cuda.is_available(),
    bf16=use_bf16,
    gradient_checkpointing=True,
    report_to=[],
)

print("TrainingArguments prontos. bf16:", use_bf16, "| fp16:", training_args.fp16)

# --------- Trainer com persistent_workers / prefetch_factor ----------
PREFETCH_FACTOR = 8  # ajuste conforme RAM
class MyTrainer(Trainer):
    def get_train_dataloader(self):
        dataset = self.train_dataset
        return DataLoader(
            dataset,
            batch_size=self.args.per_device_train_batch_size,
            shuffle=True,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
            persistent_workers=True,
            prefetch_factor=PREFETCH_FACTOR,
            collate_fn=default_data_collator,
        )

trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=default_data_collator,
)

# callbacks
trainer.add_callback(SavePeftCallback(save_dir=str(ckpt_dir / "lora_checkpoints"), save_steps=save_steps))
trainer.add_callback(CSVLoggerCallback(csv_path=str(ckpt_dir / "train_log.csv")))
print("Trainer criado com callbacks. CSV em:", ckpt_dir / "train_log.csv")

# --------- Treino + mensure ----------
start_time = time.time()
last_ckpt = get_last_checkpoint(str(ckpt_dir))
if last_ckpt:
    print("Retomando do checkpoint:", last_ckpt)
    train_out = trainer.train(resume_from_checkpoint=last_ckpt)
else:
    print("Treino do zero...")
    train_out = trainer.train()
end_time = time.time()

total_sec = end_time - start_time
global_steps = int(getattr(trainer.state, "global_step", 0))
avg_sec_per_step = (total_sec / global_steps) if global_steps else None

timing = {
    "total_seconds": total_sec,
    "global_steps": global_steps,
    "avg_sec_per_step": avg_sec_per_step,
    "dataloader_num_workers": dataloader_num_workers,
    "persistent_workers": True,
    "prefetch_factor": PREFETCH_FACTOR,
    "pretok": PRETOKENIZE,
    "packed": True,
    "timestamp_end": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(end_time)),
}
with open(ckpt_dir / "timing.json", "w", encoding="utf-8") as f:
    json.dump(timing, f, indent=2)
print("Timing salvo em:", ckpt_dir / "timing.json")
print("Total seconds:", total_sec, "| Global steps:", global_steps, "| Avg sec/step:", avg_sec_per_step)

# --------- salvar versão final do adapter (não sobrescreve) ----------
def make_versioned_adapter_dir(base_dir: Path, prefix: str = "lora-final"):
    ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    final_dir = Path(base_dir) / f"{prefix}-{ts}"
    final_dir.mkdir(parents=True, exist_ok=False)
    return final_dir

def save_final_adapter(model, tokenizer, adapter_base_dir: Path):
    base_dir = Path(adapter_base_dir); base_dir.mkdir(parents=True, exist_ok=True)
    final_dir = make_versioned_adapter_dir(base_dir)
    model.save_pretrained(str(final_dir))     # só o adapter (PEFT)
    tokenizer.save_pretrained(str(final_dir)) # prático para inferência
    print(f"[FINAL SAVE] LoRA final salvo em: {final_dir}")
    return final_dir

final_adapter_dir = save_final_adapter(trainer.model, tokenizer, ADAPTERDIR)

# --------- meta e histórico ----------
def save_training_meta(ckpt_dir: Path, adapter_final_dir: Path, training_args, peft_config, dataset_size, steps_per_epoch):
    # serialização segura de target_modules
    tm = getattr(peft_config, "target_modules", [])
    if isinstance(tm, (set, tuple)):
        tm = list(tm)
    elif not isinstance(tm, list):
        tm = [tm]

    meta = {
        "timestamp": datetime.datetime.now().isoformat(),
        "model_id": MODEL_ID,
        "max_len": MAX_LEN,
        "adapter_final_dir": str(adapter_final_dir),
        "ckpt_dir": str(ckpt_dir),
        "dataset_size": int(dataset_size),
        "steps_per_epoch": int(steps_per_epoch),
        "batch_effective": training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps,
        "training_args": {
            "per_device_train_batch_size": training_args.per_device_train_batch_size,
            "gradient_accumulation_steps": training_args.gradient_accumulation_steps,
            "learning_rate": training_args.learning_rate,
            "lr_scheduler_type": str(training_args.lr_scheduler_type),
            "logging_steps": training_args.logging_steps,
            "save_steps": training_args.save_steps,
            "save_total_limit": training_args.save_total_limit,
            "num_train_epochs": training_args.num_train_epochs,
            "fp16": bool(training_args.fp16),
            "bf16": bool(training_args.bf16),
            "dataloader_num_workers": training_args.dataloader_num_workers,
        },
        "peft_config": {
            "r": peft_config.r,
            "lora_alpha": peft_config.lora_alpha,
            "lora_dropout": float(peft_config.lora_dropout),
            "target_modules": tm,
            "bias": peft_config.bias,
            "task_type": str(peft_config.task_type),
        },
        "notes": "CSV em train_log.csv; checkpoints LoRA em lora_checkpoints/; métricas completas em training_history.json",
    }
    meta_path = Path(ckpt_dir) / "training_meta.json"
    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    print("[META] salvo em", meta_path)
    return meta_path

meta_path = save_training_meta(
    ckpt_dir=ckpt_dir,
    adapter_final_dir=final_adapter_dir,
    training_args=training_args,
    peft_config=peft_config,
    dataset_size=N,
    steps_per_epoch=steps_per_epoch,
)
print("training_meta salvo em:", meta_path)

with open(ckpt_dir / "training_history.json", "w", encoding="utf-8") as f:
    json.dump(trainer.state.log_history, f, ensure_ascii=False, indent=2)
print("training_history salvo em:", ckpt_dir / "training_history.json")

"""#Inferência comparativa (antes x depois)"""

# === CÉLULA 11 — Inferência comparativa (baseline x LoRA) ====================
from pathlib import Path
import json, re, torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel

MODEL_ID     = "TinyLlama/TinyLlama_v1.1"
ADAPTER_ROOT = Path("/content/drive/MyDrive/tech-challenge-3/LLM/tinyllama-lora")
DATASET_PATH = Path("/content/drive/MyDrive/tech-challenge-3/data/dataset_formatado_para_treino.json")

# a) usar um diretório específico (comente se quiser a opção b)
EXPLICIT_ADAPTER = ADAPTER_ROOT / "lora-final-20251007-141458"

# b) ou pegar o mais recente "lora-final-*"
cands = [p for p in ADAPTER_ROOT.glob("lora-final-*") if p.is_dir()]
ADAPTER_DIR = EXPLICIT_ADAPTER if EXPLICIT_ADAPTER.exists() else (max(cands, key=lambda p: p.stat().st_mtime) if cands else ADAPTER_ROOT)
print("Usando adapter:", ADAPTER_DIR)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
)

# Tokenizer salvo junto do adapter (garante compat. das tags especiais)
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# EOS extras para parar no fim de turno
eos_ids = [tokenizer.eos_token_id]
for t in ["<end_of_turn>", "</s>"]:
    tid = tokenizer.convert_tokens_to_ids(t)
    if tid not in (None, tokenizer.unk_token_id, -1):
        eos_ids.append(int(tid))

def generate(pipe, prompt, max_new_tokens=128):
    out = pipe(
        prompt,
        max_new_tokens=max_new_tokens,
        do_sample=True, temperature=0.7, top_p=0.9,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=eos_ids,
        return_full_text=False,   # não retornar o prompt
    )
    return out[0]["generated_text"]

# Prompts "coerentes" para cada modelo
def build_prompt_lora(title: str) -> str:
    return (
        "<bos><start_of_turn>user\n"
        "Gere uma descrição para o seguinte produto:\n"
        "### Título:\n"
        f"{title}<end_of_turn>\n"
        "<start_of_turn>model\n"
    )

def build_prompt_baseline(title: str) -> str:
    return (
        "Escreva uma descrição de produto clara e atrativa (português) para o título abaixo.\n"
        f"Título: {title}\n"
        "Descrição:"
    )

# Utilitário para pegar alguns títulos do dataset
def _read_examples(path: Path):
    with open(path, "r", encoding="utf-8") as f:
        first = f.readline()
        if first.strip().startswith("["):
            f.seek(0)
            return json.load(f)
        items = []
        if first.strip():
            items.append(json.loads(first))
        for line in f:
            if line.strip():
                items.append(json.loads(line))
        return items

TITLE_RE = re.compile(r"###\s*T[ií]tulo:\s*\n(?P<title>.+?)<end_of_turn>", re.IGNORECASE | re.DOTALL)
def extract_title(text: str):
    m = TITLE_RE.search(text)
    return m.group("title").strip() if m else None

titles = []
try:
    data = _read_examples(DATASET_PATH)
    for row in data:
        t = extract_title(row.get("text", ""))
        if t:
            titles.append(t)
        if len(titles) >= 3:
            break
except Exception as e:
    print("Aviso ao ler dataset para amostras:", e)

if not titles:
    titles = ["Girls Ballet Tutu Neon Pink", "Mog's Kittens", "Girls Ballet Tutu Neon Blue"]

# Pipelines (baseline e LoRA)
baseline_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, device_map="auto", quantization_config=bnb_config, trust_remote_code=False
)
baseline_model.config.use_cache = True
baseline_gen = pipeline("text-generation", model=baseline_model, tokenizer=tokenizer, return_full_text=True)

lora_base  = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, device_map="auto", quantization_config=bnb_config, trust_remote_code=False
)
lora_model = PeftModel.from_pretrained(lora_base, ADAPTER_DIR)
lora_gen   = pipeline("text-generation", model=lora_model, tokenizer=tokenizer, return_full_text=True)

print("lora_gen pronto ✅")
print(f"Total de títulos: {len(titles)} | Títulos:", titles)

# Execução
for i, title in enumerate(titles, 1):
    p_base = build_prompt_baseline(title)
    p_lora = build_prompt_lora(title)

    print("="*100)
    print(f"[{i}] TÍTULO: {title}\n")

    print("— Baseline —")
    print(generate(baseline_gen, p_base).strip(), "\n")

    print("— Com LoRA —")
    print(generate(lora_gen, p_lora).strip(), "\n")

"""Outros

"""

# === CÉLULA 12 ===
# Objetivo: estatísticas e inspeções finais do dataset / tokens
# Comentários:
# 1. Verifica tokens médios e recalcula steps.
# 2. Útil para ajustar MAX_LEN e batch sizes.

from datasets import Dataset

#print("Tamanho total do dataset:", len(ds))

# Seleciona as primeiras 105 mil linhas (alternativa para reduzir)
#ds_reduzido = ds.select(range(min(105_000, len(ds))))
#print("Dataset reduzido:", len(ds_reduzido))

import math
#N = len(ds)                         # dataset completo
per_device_bs = 1
grad_accum    = 8
world_size    = 1
#steps_per_epoch = math.ceil(N / (per_device_bs * world_size * grad_accum))
#print("Steps por época:", steps_per_epoch)

# medição rápida do tamanho em tokens (amostra)
#print("Tokens médios por exemplo (amostra):", ds.map(lambda x: {"len": len(tokenizer(x["text"])["input_ids"])})["len"][:100])

# === (Plot) Curva de treinamento: loss x step ===
import json
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

CKPT_DIR = Path("/content/drive/MyDrive/tech-challenge-3/LLM/ft-output/full")
CSV_LOG  = CKPT_DIR / "train_log.csv"
HIST_LOG = CKPT_DIR / "training_history.json"
TIMING   = CKPT_DIR / "timing.json"

def _load_df_from_csv_or_history(csv_path: Path, hist_path: Path) -> pd.DataFrame:
    if csv_path.exists():
        try:
            # engine='python' + on_bad_lines='skip' lida com colunas variáveis
            df = pd.read_csv(csv_path, engine="python", on_bad_lines="skip")
        except Exception:
            df = pd.DataFrame()
    else:
        df = pd.DataFrame()

    if df.empty:
        # fallback: reconstruir do training_history.json (sempre funciona)
        rows = []
        if hist_path.exists():
            with open(hist_path, "r", encoding="utf-8") as f:
                hist = json.load(f)
            for rec in hist:
                if ("loss" in rec) or ("loss_step" in rec):
                    rows.append({
                        "global_step": rec.get("global_step"),
                        "epoch": rec.get("epoch"),
                        "loss": rec.get("loss", rec.get("loss_step")),
                        "learning_rate": rec.get("learning_rate"),
                    })
        df = pd.DataFrame(rows)

    # normalização
    if "loss" not in df.columns and "loss_step" in df.columns:
        df["loss"] = pd.to_numeric(df["loss_step"], errors="coerce")
    if "global_step" in df.columns and df["global_step"].notna().any():
        df["step"] = pd.to_numeric(df["global_step"], errors="coerce").fillna(method="ffill")
    else:
        df["step"] = range(1, len(df) + 1)
    # tipos
    for col in ("step", "loss", "learning_rate"):
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    df = df.dropna(subset=["step", "loss"]).sort_values("step").reset_index(drop=True)
    return df

def _ema(series: pd.Series, span: int = 50) -> pd.Series:
    return series.ewm(span=span, adjust=False).mean()

# carrega dados
df = _load_df_from_csv_or_history(CSV_LOG, HIST_LOG)
ema_span = max(10, min(100, len(df)//50))  # janela adaptativa razoável
df["loss_ema"] = _ema(df["loss"], span=ema_span)

avg_sec_per_step = None
if TIMING.exists():
    try:
        with open(TIMING, "r", encoding="utf-8") as f:
            tinfo = json.load(f)
        avg_sec_per_step = tinfo.get("avg_sec_per_step", None)
    except Exception:
        pass

# plota
plt.figure(figsize=(10, 5))
ax = plt.gca()
ax.plot(df["step"], df["loss"], alpha=0.35, linewidth=1, label="loss (raw)")
ax.plot(df["step"], df["loss_ema"], linewidth=2, label=f"loss (EMA, span={ema_span})")
ax.set_xlabel("global step")
ax.set_ylabel("training loss")
ttl = "Training: Loss vs Step"
if avg_sec_per_step:
    ttl += f"  |  avg sec/step ≈ {avg_sec_per_step:.3f}"
ax.set_title(ttl)
ax.grid(True, linestyle="--", alpha=0.3)

# eixo secundário com LR (se houver)
if "learning_rate" in df.columns and df["learning_rate"].notna().any():
    ax2 = ax.twinx()
    ax2.plot(df["step"], df["learning_rate"], linestyle="--", alpha=0.5, label="learning rate")
    ax2.set_ylabel("learning rate")
    # legenda combinada
    lines1, labels1 = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines1 + lines2, labels1 + labels2, loc="upper right")
else:
    ax.legend(loc="upper right")

# salva figura
out_png = CKPT_DIR / "train_curve.png"
plt.tight_layout()
plt.savefig(out_png, dpi=150)
plt.show()
print("Gráfico salvo em:", out_png)