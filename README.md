# ğŸ§  Tech Challenge FASE 03: Fine-Tuning do Gemma/TinyLlama para Perguntas e Respostas de Produtos

## ğŸ¯ Objetivo do Projeto

Este projeto realiza o fine-tuning de um modelo de linguagem (LLM) para atuar como um especialista em produtos da Amazon.
O modelo foi treinado para receber o tÃ­tulo de um produto e gerar uma descriÃ§Ã£o detalhada e criativa, com base no dataset "The Amazon Titles-1.3MM".

O foco do desafio Ã© demonstrar a capacidade de adaptar um modelo de linguagem prÃ©-treinado (como o TinyLlama ou Gemma) para um domÃ­nio especÃ­fico de e-commerce, otimizando custo e performance usando LoRA + QLoRA (quantizaÃ§Ã£o 4-bit).

## ğŸ› ï¸ Tecnologias e Modelos Utilizados

- **Modelo Base:** [TinyLlama/TinyLlama_v1.1](https://huggingface.co/TinyLlama/TinyLlama_v1.1)
- **Alternativa Gemma:** [gemma-3-4b-it](https://huggingface.co/google/gemma-3-4b-it)
- **Dataset:** The Amazon Titles-1.3MM (`trn.json`)
- **Bibliotecas Principais:**
  - transformers
  - trl
  - peft
  - bitsandbytes
  - datasets
  - accelerate
  - sentencepiece
- **Ambiente de Treinamento:** Google Colab Pro (GPU A100)

## ğŸ“š LLM Models e ReferÃªncias

- [Hugging Face Hub](https://huggingface.co/)
- [TinyLlama v1.1](https://huggingface.co/TinyLlama/TinyLlama_v1.1)
- [Gemma 3 4B IT](https://huggingface.co/google/gemma-3-4b-it)
- [VersÃ£o Quantizada GGUF](https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF)

## ğŸ“‚ Estrutura do RepositÃ³rio

```
tech-challenge-3/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trn.json                           # Dataset original
â”‚   â”œâ”€â”€ dataset_formatado_para_treino.json # Dataset jÃ¡ formatado para SFT
â”œâ”€â”€ LLM/
â”‚   â”œâ”€â”€ ft-output/             # Checkpoints do treinamento (retomada automÃ¡tica)
â”‚   â””â”€â”€ tinyllama-lora/        # Adapters LoRA salvos
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ tech_challenge_3.ipynb # Notebook principal (Google Colab)
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ tree.txt
```

## ğŸš€ Como Executar

O treinamento completo foi implementado no notebook `notebooks/tech_challenge_3.ipynb`.
Para reproduzir o projeto, basta abrir o notebook no Google Colab e seguir as cÃ©lulas na ordem.

### ğŸ”¹ Etapas Principais

1. **Montar o Google Drive**
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
   ```

2. **Instalar dependÃªncias**
   ```bash
   pip install -q -U \
   "transformers>=4.38" \
   "trl>=0.8.0" \
   "peft>=0.9.0" \
   "bitsandbytes>=0.41" \
   "datasets" \
   "accelerate>=0.28" \
   "sentencepiece" \
   "pyarrow<20.0"
   ```

3. **Login no Hugging Face**
   ```python
   from huggingface_hub import login
   login()  # Insira seu token HF
   ```

4. **Executar o Fine-tuning**
   O notebook jÃ¡ contÃ©m o pipeline completo de:
   - carregamento do modelo,
   - formataÃ§Ã£o do dataset,
   - treinamento com LoRA + QLoRA,
   - salvamento automÃ¡tico de checkpoints.

5. **Salvar o modelo**
   ApÃ³s o treinamento, os adapters LoRA e o tokenizer sÃ£o salvos em:
   ```
   /LLM/tinyllama-lora/
   ```

## ğŸ’¾ Checkpoints e Retomada

O script identifica automaticamente o Ãºltimo checkpoint salvo no diretÃ³rio de saÃ­da e retoma o treino de onde parou.
Ideal para casos de interrupÃ§Ã£o do Colab.

## ğŸ§ª AvaliaÃ§Ã£o

### Exemplo de Prompt:
```
Gere uma descriÃ§Ã£o criativa para o produto:
TÃ­tulo: Girls Ballet Tutu Neon Pink
```

**SaÃ­da (Baseline):**
> Um vestido simples rosa para meninas.

**SaÃ­da (Modelo Fine-tunado):**
> Um tutu rosa neon encantador, perfeito para pequenas bailarinas brilharem nos palcos e ensaios.

## ğŸ“Š HiperparÃ¢metros Principais

| ParÃ¢metro | Valor | DescriÃ§Ã£o |
|------------|--------|------------|
| `learning_rate` | 2e-4 | Taxa de aprendizado |
| `batch_size` | 1 | Tamanho do batch |
| `gradient_accumulation_steps` | 8 | AcÃºmulo de gradientes |
| `max_seq_length` | 256 | Tokens por exemplo |
| `num_train_epochs` | 1 | Quantas vezes percorre o dataset |
| `packing` | True | Junta exemplos atÃ© o limite de tokens |
| `save_steps` | 2000 | Checkpoints periÃ³dicos |

## ğŸ“ˆ Resultados e ConclusÃµes

âœ… O modelo aprendeu a gerar descriÃ§Ãµes mais criativas e ricas em contexto.  
âœ… O uso de QLoRA (4-bit) reduziu o consumo de memÃ³ria sem perda perceptÃ­vel de qualidade.  
âœ… O packing=True acelerou o treinamento em cerca de 30%.  
âœ… O training loss foi decrescendo gradualmente, indicando aprendizado estÃ¡vel.  
âœ… Os checkpoints automÃ¡ticos garantiram retomada segura apÃ³s quedas do Colab.

## ğŸ“¹ VÃ­deo Explicativo

ğŸ¥ (Adicionar link do vÃ­deo explicativo do projeto no YouTube quando disponÃ­vel.)

## ğŸ‘¨â€ğŸ’» Autor

**Seu Nome**  
Desenvolvedor | IA & Machine Learning  
ğŸ“§ [seuemail@exemplo.com](mailto:seuemail@exemplo.com)  
ğŸ”— [LinkedIn](https://linkedin.com/in/seu-perfil) | [GitHub](https://github.com/seu-usuario)
